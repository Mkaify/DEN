{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Advanced Heart Disease Risk Assessment: A Comprehensive ML Pipeline\n",
        "\n",
        "**Author:** Data Science Team  \n",
        "**Date:** January 2024  \n",
        "**Version:** 2.0\n",
        "\n",
        "---\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "This project develops a sophisticated machine learning pipeline for cardiovascular disease risk assessment using advanced ensemble methods and feature engineering techniques. Unlike traditional approaches, this study emphasizes:\n",
        "\n",
        "- **Advanced Feature Engineering**: Creating clinically meaningful derived features\n",
        "- **Ensemble Learning**: Implementing stacking and voting classifiers\n",
        "- **Clinical Interpretation**: SHAP analysis for model explainability\n",
        "- **Risk Stratification**: Multi-class risk categorization beyond binary classification\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Project Objectives\n",
        "\n",
        "1. **Primary Goal**: Develop a robust ML pipeline for heart disease risk prediction\n",
        "2. **Secondary Goals**: \n",
        "   - Identify key cardiovascular risk factors through advanced analytics\n",
        "   - Create interpretable models for clinical decision support\n",
        "   - Implement risk stratification beyond binary classification\n",
        "   - Validate model performance using clinical metrics\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìö Library Imports and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import sys\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Machine Learning - Core\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "\n",
        "# Machine Learning - Models\n",
        "from sklearn.ensemble import (\n",
        "    GradientBoostingClassifier, \n",
        "    VotingClassifier, \n",
        "    StackingClassifier,\n",
        "    ExtraTreesClassifier\n",
        ")\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# For comprehensive model options\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    from lightgbm import LGBMClassifier\n",
        "    advanced_models_available = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è XGBoost/LightGBM not available - using sklearn models only\")\n",
        "    advanced_models_available = False\n",
        "\n",
        "# Evaluation and Metrics\n",
        "from sklearn.metrics import (\n",
        "    classification_report, \n",
        "    confusion_matrix, \n",
        "    roc_auc_score, \n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    average_precision_score,\n",
        "    make_scorer,\n",
        "    accuracy_score, \n",
        "    precision_score, \n",
        "    recall_score, \n",
        "    f1_score\n",
        ")\n",
        "\n",
        "# Statistical Analysis\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Configuration\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"üìä Environment: Python {sys.version.split()[0]}\")\n",
        "print(f\"üöÄ Advanced models available: {advanced_models_available}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìä Data Acquisition and Comprehensive Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the cardiovascular dataset\n",
        "# For this demonstration, we'll use the heart.csv file\n",
        "try:\n",
        "    df_raw = pd.read_csv('heart.csv')\n",
        "    print(\"‚úÖ Dataset loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ö†Ô∏è heart.csv not found. Please ensure the dataset is in the current directory.\")\n",
        "    print(\"Creating sample dataset for demonstration...\")\n",
        "    # Create a small sample dataset if file not found\n",
        "    np.random.seed(42)\n",
        "    n_samples = 300\n",
        "    df_raw = pd.DataFrame({\n",
        "        'age': np.random.randint(30, 80, n_samples),\n",
        "        'sex': np.random.randint(0, 2, n_samples),\n",
        "        'cp': np.random.randint(0, 4, n_samples),\n",
        "        'trestbps': np.random.randint(90, 200, n_samples),\n",
        "        'chol': np.random.randint(120, 400, n_samples),\n",
        "        'fbs': np.random.randint(0, 2, n_samples),\n",
        "        'restecg': np.random.randint(0, 3, n_samples),\n",
        "        'thalach': np.random.randint(80, 200, n_samples),\n",
        "        'exang': np.random.randint(0, 2, n_samples),\n",
        "        'oldpeak': np.random.uniform(0, 6, n_samples),\n",
        "        'slope': np.random.randint(0, 3, n_samples),\n",
        "        'ca': np.random.randint(0, 5, n_samples),\n",
        "        'thal': np.random.randint(1, 4, n_samples),\n",
        "        'target': np.random.randint(0, 2, n_samples)\n",
        "    })\n",
        "    print(\"‚úÖ Sample dataset created!\")\n",
        "\n",
        "# Create a copy for processing\n",
        "df = df_raw.copy()\n",
        "\n",
        "print(f\"üìà Dataset Shape: {df.shape}\")\n",
        "print(f\"üè• Total Patients: {len(df):,}\")\n",
        "print(f\"üî¨ Features Available: {df.shape[1]-1}\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nüìã Dataset Preview:\")\n",
        "display(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced data profiling function\n",
        "def comprehensive_data_profile(dataframe):\n",
        "    \"\"\"\n",
        "    Generate comprehensive data profiling report\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üìã COMPREHENSIVE DATA PROFILING REPORT\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Basic Information\n",
        "    print(\"\\nüèóÔ∏è DATASET STRUCTURE:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Rows: {dataframe.shape[0]:,}\")\n",
        "    print(f\"Columns: {dataframe.shape[1]}\")\n",
        "    print(f\"Memory Usage: {dataframe.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    \n",
        "    # Data Types Analysis\n",
        "    print(\"\\nüìä DATA TYPES DISTRIBUTION:\")\n",
        "    print(\"-\" * 40)\n",
        "    dtype_counts = dataframe.dtypes.value_counts()\n",
        "    for dtype, count in dtype_counts.items():\n",
        "        print(f\"{dtype}: {count} columns\")\n",
        "    \n",
        "    # Missing Values Analysis\n",
        "    print(\"\\n‚ùå MISSING VALUES ANALYSIS:\")\n",
        "    print(\"-\" * 40)\n",
        "    missing_data = dataframe.isnull().sum()\n",
        "    missing_percent = (missing_data / len(dataframe)) * 100\n",
        "    \n",
        "    if missing_data.sum() == 0:\n",
        "        print(\"‚úÖ No missing values detected!\")\n",
        "    else:\n",
        "        missing_df = pd.DataFrame({\n",
        "            'Missing Count': missing_data[missing_data > 0],\n",
        "            'Percentage': missing_percent[missing_percent > 0]\n",
        "        }).sort_values('Percentage', ascending=False)\n",
        "        print(missing_df)\n",
        "    \n",
        "    # Duplicate Analysis\n",
        "    print(\"\\nüîÑ DUPLICATE ANALYSIS:\")\n",
        "    print(\"-\" * 40)\n",
        "    duplicates = dataframe.duplicated().sum()\n",
        "    print(f\"Duplicate rows: {duplicates} ({duplicates/len(dataframe)*100:.2f}%)\")\n",
        "    \n",
        "    return dataframe.describe(include='all')\n",
        "\n",
        "# Generate comprehensive profile\n",
        "desc_stats = comprehensive_data_profile(df)\n",
        "display(desc_stats)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### üè• Clinical Feature Mapping and Domain Knowledge Integration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clinical feature mapping with medical interpretation\n",
        "FEATURE_MAPPING = {\n",
        "    'age': 'Patient Age (years)',\n",
        "    'sex': 'Biological Sex (1=Male, 0=Female)',\n",
        "    'cp': 'Chest Pain Type (0-3)',\n",
        "    'trestbps': 'Resting Blood Pressure (mmHg)',\n",
        "    'chol': 'Serum Cholesterol (mg/dl)',\n",
        "    'fbs': 'Fasting Blood Sugar >120 mg/dl (1=Yes)',\n",
        "    'restecg': 'Resting ECG Results (0-2)',\n",
        "    'thalach': 'Maximum Heart Rate Achieved',\n",
        "    'exang': 'Exercise Induced Angina (1=Yes)',\n",
        "    'oldpeak': 'ST Depression Induced by Exercise',\n",
        "    'slope': 'Slope of Peak Exercise ST Segment (0-2)',\n",
        "    'ca': 'Number of Major Vessels (0-4)',\n",
        "    'thal': 'Thalassemia Type (1-3)',\n",
        "    'target': 'Heart Disease Presence (1=Disease, 0=No Disease)'\n",
        "}\n",
        "\n",
        "# Clinical risk thresholds (based on medical literature)\n",
        "CLINICAL_THRESHOLDS = {\n",
        "    'age_high_risk': 65,\n",
        "    'chol_high': 240,\n",
        "    'chol_borderline': 200,\n",
        "    'trestbps_high': 140,\n",
        "    'trestbps_elevated': 130,\n",
        "    'thalach_low_fitness': 100\n",
        "}\n",
        "\n",
        "# Display feature information\n",
        "feature_df = pd.DataFrame([\n",
        "    {'Feature': k, 'Clinical Meaning': v} \n",
        "    for k, v in FEATURE_MAPPING.items()\n",
        "])\n",
        "\n",
        "print(\"üè• CLINICAL FEATURE REFERENCE:\")\n",
        "print(\"=\" * 80)\n",
        "display(feature_df)\n",
        "\n",
        "# Check target distribution\n",
        "target_dist = df['target'].value_counts().sort_index()\n",
        "print(\"\\n‚ù§Ô∏è HEART DISEASE DISTRIBUTION:\")\n",
        "print(\"-\" * 40)\n",
        "for val, count in target_dist.items():\n",
        "    label = \"No Disease\" if val == 0 else \"Disease Present\"\n",
        "    percentage = (count / len(df)) * 100\n",
        "    print(f\"{label}: {count:,} patients ({percentage:.1f}%)\")\n",
        "\n",
        "# Basic statistics by target\n",
        "print(\"\\nüìä BASIC STATISTICS BY DISEASE STATUS:\")\n",
        "print(\"-\" * 50)\n",
        "for feature in ['age', 'chol', 'trestbps', 'thalach']:\n",
        "    if feature in df.columns:\n",
        "        no_disease_mean = df[df['target'] == 0][feature].mean()\n",
        "        disease_mean = df[df['target'] == 1][feature].mean()\n",
        "        print(f\"{feature.upper()}: No Disease={no_disease_mean:.1f}, Disease={disease_mean:.1f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üî¨ Advanced Feature Engineering Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdvancedFeatureEngineer:\n",
        "    \"\"\"\n",
        "    Advanced feature engineering for cardiovascular risk assessment\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.features_created = []\n",
        "    \n",
        "    def create_clinical_risk_features(self, df):\n",
        "        \"\"\"\n",
        "        Create clinically meaningful risk stratification features\n",
        "        \"\"\"\n",
        "        df_enhanced = df.copy()\n",
        "        \n",
        "        # Age-based risk stratification\n",
        "        df_enhanced['age_risk_category'] = pd.cut(\n",
        "            df_enhanced['age'], \n",
        "            bins=[0, 40, 55, 65, 100], \n",
        "            labels=['Low', 'Moderate', 'High', 'Very High']\n",
        "        )\n",
        "        \n",
        "        # Cholesterol risk categories (ATP III guidelines)\n",
        "        df_enhanced['chol_risk'] = pd.cut(\n",
        "            df_enhanced['chol'],\n",
        "            bins=[0, 200, 240, 1000],\n",
        "            labels=['Desirable', 'Borderline', 'High']\n",
        "        )\n",
        "        \n",
        "        # Blood pressure categories (AHA guidelines)\n",
        "        df_enhanced['bp_category'] = pd.cut(\n",
        "            df_enhanced['trestbps'],\n",
        "            bins=[0, 120, 130, 140, 300],\n",
        "            labels=['Normal', 'Elevated', 'Stage1_HTN', 'Stage2_HTN']\n",
        "        )\n",
        "        \n",
        "        # Heart rate reserve (fitness indicator)\n",
        "        max_hr_predicted = 220 - df_enhanced['age']\n",
        "        df_enhanced['hr_reserve'] = max_hr_predicted - df_enhanced['thalach']\n",
        "        df_enhanced['hr_reserve_percent'] = (df_enhanced['thalach'] / max_hr_predicted) * 100\n",
        "        \n",
        "        # Exercise capacity assessment\n",
        "        df_enhanced['exercise_capacity'] = np.where(\n",
        "            (df_enhanced['thalach'] >= 150) & (df_enhanced['exang'] == 0),\n",
        "            'Good',\n",
        "            np.where(\n",
        "                (df_enhanced['thalach'] >= 120) & (df_enhanced['exang'] == 0),\n",
        "                'Fair',\n",
        "                'Poor'\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        # Metabolic risk score\n",
        "        df_enhanced['metabolic_risk_score'] = (\n",
        "            (df_enhanced['chol'] > 240).astype(int) * 2 +\n",
        "            (df_enhanced['fbs'] == 1).astype(int) * 2 +\n",
        "            (df_enhanced['trestbps'] > 140).astype(int) * 3\n",
        "        )\n",
        "        \n",
        "        # Composite cardiovascular risk index\n",
        "        df_enhanced['cv_risk_index'] = (\n",
        "            (df_enhanced['age'] / 100) * 0.3 +\n",
        "            (df_enhanced['chol'] / 400) * 0.2 +\n",
        "            (df_enhanced['trestbps'] / 200) * 0.2 +\n",
        "            (df_enhanced['oldpeak'] / 10) * 0.15 +\n",
        "            (df_enhanced['ca'] / 4) * 0.15\n",
        "        )\n",
        "        \n",
        "        self.features_created.extend([\n",
        "            'age_risk_category', 'chol_risk', 'bp_category',\n",
        "            'hr_reserve', 'hr_reserve_percent', 'exercise_capacity',\n",
        "            'metabolic_risk_score', 'cv_risk_index'\n",
        "        ])\n",
        "        \n",
        "        return df_enhanced\n",
        "    \n",
        "    def create_interaction_features(self, df):\n",
        "        \"\"\"\n",
        "        Create meaningful feature interactions\n",
        "        \"\"\"\n",
        "        df_interactions = df.copy()\n",
        "        \n",
        "        # Age-gender interaction (different risk profiles)\n",
        "        df_interactions['age_sex_interaction'] = df_interactions['age'] * df_interactions['sex']\n",
        "        \n",
        "        # Cholesterol-age interaction\n",
        "        df_interactions['chol_age_ratio'] = df_interactions['chol'] / df_interactions['age']\n",
        "        \n",
        "        # Exercise response (thalach vs oldpeak)\n",
        "        df_interactions['exercise_response'] = df_interactions['thalach'] / (df_interactions['oldpeak'] + 1)\n",
        "        \n",
        "        # Vascular health score\n",
        "        df_interactions['vascular_health'] = (\n",
        "            df_interactions['thalach'] / df_interactions['trestbps']\n",
        "        )\n",
        "        \n",
        "        self.features_created.extend([\n",
        "            'age_sex_interaction', 'chol_age_ratio', \n",
        "            'exercise_response', 'vascular_health'\n",
        "        ])\n",
        "        \n",
        "        return df_interactions\n",
        "\n",
        "# Initialize and apply feature engineering\n",
        "feature_engineer = AdvancedFeatureEngineer()\n",
        "\n",
        "print(\"üî¨ ADVANCED FEATURE ENGINEERING PIPELINE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Apply clinical risk features\n",
        "print(\"\\n‚öïÔ∏è Creating clinical risk stratification features...\")\n",
        "df_clinical = feature_engineer.create_clinical_risk_features(df)\n",
        "\n",
        "# Apply interaction features\n",
        "print(\"üîÑ Creating feature interactions...\")\n",
        "df_engineered = feature_engineer.create_interaction_features(df_clinical)\n",
        "\n",
        "print(f\"\\n‚úÖ Feature engineering complete!\")\n",
        "print(f\"üìä Original features: {df.shape[1]}\")\n",
        "print(f\"üöÄ Enhanced features: {df_engineered.shape[1]}\")\n",
        "print(f\"‚ûï New features created: {len(feature_engineer.features_created)}\")\n",
        "\n",
        "# Display new features created\n",
        "print(\"\\nüÜï NEW FEATURES CREATED:\")\n",
        "for i, feature in enumerate(feature_engineer.features_created, 1):\n",
        "    print(f\"{i:2d}. {feature}\")\n",
        "\n",
        "# Show sample of new features\n",
        "print(\"\\nüìã SAMPLE OF ENGINEERED FEATURES:\")\n",
        "print(\"-\" * 50)\n",
        "sample_cols = ['age', 'age_risk_category', 'chol', 'chol_risk', 'cv_risk_index', 'metabolic_risk_score']\n",
        "display(df_engineered[sample_cols].head())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìà Advanced Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced EDA with clinical insights\n",
        "def analyze_clinical_risk_factors(df):\n",
        "    \"\"\"\n",
        "    Analyze clinical risk factors with statistical significance\n",
        "    \"\"\"\n",
        "    print(\"‚öïÔ∏è CLINICAL RISK FACTOR ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Age risk analysis\n",
        "    age_risk_crosstab = pd.crosstab(df['age_risk_category'], df['target'])\n",
        "    chi2, p_value, _, _ = chi2_contingency(age_risk_crosstab)\n",
        "    \n",
        "    print(\"\\nüìä AGE RISK STRATIFICATION:\")\n",
        "    print(\"-\" * 40)\n",
        "    age_risk_pct = pd.crosstab(df['age_risk_category'], df['target'], normalize='index') * 100\n",
        "    print(age_risk_pct.round(1))\n",
        "    print(f\"\\nüìà Chi-square test p-value: {p_value:.2e}\")\n",
        "    \n",
        "    # Cholesterol risk analysis\n",
        "    print(\"\\nüß™ CHOLESTEROL RISK ANALYSIS:\")\n",
        "    print(\"-\" * 40)\n",
        "    chol_risk_pct = pd.crosstab(df['chol_risk'], df['target'], normalize='index') * 100\n",
        "    print(chol_risk_pct.round(1))\n",
        "    \n",
        "    # Exercise capacity analysis\n",
        "    print(\"\\nüèÉ EXERCISE CAPACITY ASSESSMENT:\")\n",
        "    print(\"-\" * 40)\n",
        "    exercise_risk_pct = pd.crosstab(df['exercise_capacity'], df['target'], normalize='index') * 100\n",
        "    print(exercise_risk_pct.round(1))\n",
        "    \n",
        "    return age_risk_crosstab, chol_risk_pct, exercise_risk_pct\n",
        "\n",
        "# Execute advanced EDA\n",
        "risk_analysis = analyze_clinical_risk_factors(df_engineered)\n",
        "\n",
        "# Create comprehensive visualization dashboard\n",
        "def create_eda_visualizations(df):\n",
        "    \"\"\"\n",
        "    Create comprehensive EDA visualizations\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('Advanced Cardiovascular Risk Analysis Dashboard', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Age distribution by heart disease\n",
        "    ax1 = axes[0, 0]\n",
        "    for target_val in df['target'].unique():\n",
        "        subset = df[df['target'] == target_val]\n",
        "        label = \"No Disease\" if target_val == 0 else \"Heart Disease\"\n",
        "        ax1.hist(subset['age'], alpha=0.7, label=label, bins=15)\n",
        "    ax1.set_xlabel('Age (years)')\n",
        "    ax1.set_ylabel('Frequency')\n",
        "    ax1.set_title('Age Distribution by Disease Status')\n",
        "    ax1.legend()\n",
        "    ax1.grid(alpha=0.3)\n",
        "    \n",
        "    # 2. Cholesterol vs Blood Pressure Risk Matrix\n",
        "    ax2 = axes[0, 1]\n",
        "    colors = ['green' if x == 0 else 'red' for x in df['target']]\n",
        "    ax2.scatter(df['chol'], df['trestbps'], c=colors, alpha=0.6)\n",
        "    ax2.axhline(y=140, color='orange', linestyle='--', alpha=0.7, label='HTN Threshold')\n",
        "    ax2.axvline(x=240, color='purple', linestyle='--', alpha=0.7, label='High Cholesterol')\n",
        "    ax2.set_xlabel('Cholesterol (mg/dl)')\n",
        "    ax2.set_ylabel('Resting BP (mmHg)')\n",
        "    ax2.set_title('Cholesterol vs BP Risk Matrix')\n",
        "    ax2.legend()\n",
        "    ax2.grid(alpha=0.3)\n",
        "    \n",
        "    # 3. Exercise Capacity Assessment\n",
        "    ax3 = axes[0, 2]\n",
        "    exercise_counts = pd.crosstab(df['exercise_capacity'], df['target'])\n",
        "    exercise_counts.plot(kind='bar', ax=ax3, color=['lightgreen', 'lightcoral'])\n",
        "    ax3.set_title('Exercise Capacity vs Heart Disease')\n",
        "    ax3.set_xlabel('Exercise Capacity')\n",
        "    ax3.set_ylabel('Count')\n",
        "    ax3.legend(['No Disease', 'Heart Disease'])\n",
        "    ax3.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 4. Cardiovascular Risk Index Distribution\n",
        "    ax4 = axes[1, 0]\n",
        "    for target_val in df['target'].unique():\n",
        "        subset = df[df['target'] == target_val]\n",
        "        label = \"No Disease\" if target_val == 0 else \"Heart Disease\"\n",
        "        ax4.hist(subset['cv_risk_index'], alpha=0.7, label=label, bins=15)\n",
        "    ax4.set_xlabel('CV Risk Index')\n",
        "    ax4.set_ylabel('Frequency')\n",
        "    ax4.set_title('Cardiovascular Risk Index Distribution')\n",
        "    ax4.legend()\n",
        "    ax4.grid(alpha=0.3)\n",
        "    \n",
        "    # 5. Metabolic Risk Score Analysis\n",
        "    ax5 = axes[1, 1]\n",
        "    metabolic_counts = pd.crosstab(df['metabolic_risk_score'], df['target'])\n",
        "    metabolic_counts.plot(kind='bar', ax=ax5, color=['lightblue', 'orange'])\n",
        "    ax5.set_title('Metabolic Risk Score Analysis')\n",
        "    ax5.set_xlabel('Metabolic Risk Score')\n",
        "    ax5.set_ylabel('Count')\n",
        "    ax5.legend(['No Disease', 'Heart Disease'])\n",
        "    \n",
        "    # 6. Correlation Heatmap of Key Features\n",
        "    ax6 = axes[1, 2]\n",
        "    key_features = ['age', 'chol', 'trestbps', 'thalach', 'oldpeak', \n",
        "                   'cv_risk_index', 'metabolic_risk_score', 'target']\n",
        "    corr_matrix = df[key_features].corr()\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=ax6, \n",
        "                fmt='.2f', square=True)\n",
        "    ax6.set_title('Feature Correlation Matrix')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# Create comprehensive visualizations\n",
        "print(\"\\nüìä GENERATING COMPREHENSIVE VISUALIZATION DASHBOARD\")\n",
        "print(\"=\" * 80)\n",
        "eda_visualizations = create_eda_visualizations(df_engineered)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### üéØ Advanced Feature Selection Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdvancedFeatureSelector:\n",
        "    \"\"\"\n",
        "    Multi-method feature selection for optimal model performance\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.selected_features = {}\n",
        "        self.feature_scores = {}\n",
        "    \n",
        "    def prepare_features(self, df, target_col='target'):\n",
        "        \"\"\"\n",
        "        Prepare features for selection (handle categorical variables)\n",
        "        \"\"\"\n",
        "        df_processed = df.copy()\n",
        "        \n",
        "        # Encode categorical variables\n",
        "        categorical_cols = df_processed.select_dtypes(include=['object', 'category']).columns\n",
        "        \n",
        "        for col in categorical_cols:\n",
        "            if col != target_col:\n",
        "                le = LabelEncoder()\n",
        "                df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
        "        \n",
        "        # Separate features and target\n",
        "        X = df_processed.drop(target_col, axis=1)\n",
        "        y = df_processed[target_col]\n",
        "        \n",
        "        return X, y\n",
        "    \n",
        "    def univariate_selection(self, X, y, k=15):\n",
        "        \"\"\"\n",
        "        Univariate feature selection using f_classif\n",
        "        \"\"\"\n",
        "        selector = SelectKBest(score_func=f_classif, k=k)\n",
        "        X_selected = selector.fit_transform(X, y)\n",
        "        \n",
        "        selected_features = X.columns[selector.get_support()].tolist()\n",
        "        feature_scores = dict(zip(X.columns, selector.scores_))\n",
        "        \n",
        "        self.selected_features['univariate'] = selected_features\n",
        "        self.feature_scores['univariate'] = feature_scores\n",
        "        \n",
        "        return selected_features, feature_scores\n",
        "    \n",
        "    def recursive_feature_elimination(self, X, y, n_features=15):\n",
        "        \"\"\"\n",
        "        Recursive Feature Elimination with ExtraTreesClassifier\n",
        "        \"\"\"\n",
        "        estimator = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "        selector = RFE(estimator, n_features_to_select=n_features, step=1)\n",
        "        X_selected = selector.fit_transform(X, y)\n",
        "        \n",
        "        selected_features = X.columns[selector.get_support()].tolist()\n",
        "        feature_rankings = dict(zip(X.columns, selector.ranking_))\n",
        "        \n",
        "        self.selected_features['rfe'] = selected_features\n",
        "        self.feature_scores['rfe'] = feature_rankings\n",
        "        \n",
        "        return selected_features, feature_rankings\n",
        "    \n",
        "    def feature_importance_selection(self, X, y, threshold=0.01):\n",
        "        \"\"\"\n",
        "        Feature selection based on tree-based feature importance\n",
        "        \"\"\"\n",
        "        # Use multiple tree-based models\n",
        "        models = {\n",
        "            'extra_trees': ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
        "            'gradient_boost': GradientBoostingClassifier(random_state=42)\n",
        "        }\n",
        "        \n",
        "        importance_scores = pd.DataFrame(index=X.columns)\n",
        "        \n",
        "        for name, model in models.items():\n",
        "            model.fit(X, y)\n",
        "            importance_scores[name] = model.feature_importances_\n",
        "        \n",
        "        # Average importance across models\n",
        "        importance_scores['mean_importance'] = importance_scores.mean(axis=1)\n",
        "        \n",
        "        # Select features above threshold\n",
        "        selected_features = importance_scores[\n",
        "            importance_scores['mean_importance'] > threshold\n",
        "        ].index.tolist()\n",
        "        \n",
        "        self.selected_features['importance'] = selected_features\n",
        "        self.feature_scores['importance'] = importance_scores['mean_importance'].to_dict()\n",
        "        \n",
        "        return selected_features, importance_scores\n",
        "    \n",
        "    def ensemble_feature_selection(self, X, y):\n",
        "        \"\"\"\n",
        "        Combine multiple selection methods for robust feature selection\n",
        "        \"\"\"\n",
        "        # Run all selection methods\n",
        "        univariate_features, _ = self.univariate_selection(X, y)\n",
        "        rfe_features, _ = self.recursive_feature_elimination(X, y)\n",
        "        importance_features, _ = self.feature_importance_selection(X, y)\n",
        "        \n",
        "        # Find consensus features (appearing in at least 2 methods)\n",
        "        all_features = set(univariate_features + rfe_features + importance_features)\n",
        "        \n",
        "        feature_votes = {}\n",
        "        for feature in all_features:\n",
        "            votes = 0\n",
        "            if feature in univariate_features:\n",
        "                votes += 1\n",
        "            if feature in rfe_features:\n",
        "                votes += 1\n",
        "            if feature in importance_features:\n",
        "                votes += 1\n",
        "            feature_votes[feature] = votes\n",
        "        \n",
        "        # Select features with 2+ votes\n",
        "        consensus_features = [f for f, votes in feature_votes.items() if votes >= 2]\n",
        "        \n",
        "        self.selected_features['ensemble'] = consensus_features\n",
        "        self.feature_scores['ensemble'] = feature_votes\n",
        "        \n",
        "        return consensus_features, feature_votes\n",
        "\n",
        "# Initialize feature selector\n",
        "feature_selector = AdvancedFeatureSelector()\n",
        "\n",
        "print(\"üéØ ADVANCED FEATURE SELECTION PIPELINE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Prepare features\n",
        "print(\"\\nüîß Preparing features for selection...\")\n",
        "X, y = feature_selector.prepare_features(df_engineered)\n",
        "print(f\"üìä Total features available: {X.shape[1]}\")\n",
        "\n",
        "# Run ensemble feature selection\n",
        "print(\"\\nü§ñ Running ensemble feature selection...\")\n",
        "selected_features, feature_votes = feature_selector.ensemble_feature_selection(X, y)\n",
        "\n",
        "print(f\"\\n‚úÖ Feature selection complete!\")\n",
        "print(f\"üéØ Selected features: {len(selected_features)}\")\n",
        "\n",
        "# Display selected features\n",
        "print(\"\\nüèÜ FINAL SELECTED FEATURES:\")\n",
        "print(\"-\" * 40)\n",
        "for i, feature in enumerate(selected_features, 1):\n",
        "    votes = feature_votes[feature]\n",
        "    print(f\"{i:2d}. {feature:<25} (votes: {votes}/3)\")\n",
        "\n",
        "# Create final dataset with selected features\n",
        "X_selected = X[selected_features]\n",
        "print(f\"\\nüìä Final dataset shape: {X_selected.shape}\")\n",
        "\n",
        "# Show feature selection summary\n",
        "print(\"\\nüìà FEATURE SELECTION METHODS SUMMARY:\")\n",
        "print(\"-\" * 50)\n",
        "for method, features in feature_selector.selected_features.items():\n",
        "    print(f\"{method.title()}: {len(features)} features selected\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ü§ñ Advanced Machine Learning Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdvancedMLPipeline:\n",
        "    \"\"\"\n",
        "    Comprehensive ML pipeline with ensemble methods and advanced evaluation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, random_state=42):\n",
        "        self.random_state = random_state\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "        self.best_model = None\n",
        "    \n",
        "    def initialize_models(self):\n",
        "        \"\"\"\n",
        "        Initialize diverse set of advanced models\n",
        "        \"\"\"\n",
        "        self.models = {\n",
        "            'Gradient Boosting': GradientBoostingClassifier(\n",
        "                n_estimators=200,\n",
        "                max_depth=5,\n",
        "                learning_rate=0.1,\n",
        "                random_state=self.random_state\n",
        "            ),\n",
        "            \n",
        "            'Extra Trees': ExtraTreesClassifier(\n",
        "                n_estimators=200,\n",
        "                max_depth=10,\n",
        "                random_state=self.random_state\n",
        "            ),\n",
        "            \n",
        "            'SVM (RBF)': SVC(\n",
        "                kernel='rbf',\n",
        "                C=1.0,\n",
        "                gamma='scale',\n",
        "                probability=True,\n",
        "                random_state=self.random_state\n",
        "            ),\n",
        "            \n",
        "            'Neural Network': MLPClassifier(\n",
        "                hidden_layer_sizes=(100, 50),\n",
        "                activation='relu',\n",
        "                solver='adam',\n",
        "                alpha=0.001,\n",
        "                learning_rate='adaptive',\n",
        "                max_iter=1000,\n",
        "                random_state=self.random_state\n",
        "            ),\n",
        "            \n",
        "            'Naive Bayes': GaussianNB()\n",
        "        }\n",
        "        \n",
        "        # Add advanced models if available\n",
        "        if advanced_models_available:\n",
        "            self.models.update({\n",
        "                'XGBoost': XGBClassifier(\n",
        "                    n_estimators=200,\n",
        "                    max_depth=6,\n",
        "                    learning_rate=0.1,\n",
        "                    random_state=self.random_state,\n",
        "                    eval_metric='logloss'\n",
        "                ),\n",
        "                \n",
        "                'LightGBM': LGBMClassifier(\n",
        "                    n_estimators=200,\n",
        "                    max_depth=6,\n",
        "                    learning_rate=0.1,\n",
        "                    random_state=self.random_state,\n",
        "                    verbose=-1\n",
        "                )\n",
        "            })\n",
        "    \n",
        "    def create_ensemble_models(self):\n",
        "        \"\"\"\n",
        "        Create advanced ensemble models\n",
        "        \"\"\"\n",
        "        # Select base models for ensembles\n",
        "        base_models = ['Gradient Boosting', 'Extra Trees', 'SVM (RBF)']\n",
        "        if advanced_models_available:\n",
        "            base_models.extend(['XGBoost', 'LightGBM'])\n",
        "        \n",
        "        # Voting Classifier\n",
        "        voting_estimators = [(name.lower().replace(' ', '_'), self.models[name]) \n",
        "                            for name in base_models[:4]]  # Use top 4 models\n",
        "        \n",
        "        voting_clf = VotingClassifier(\n",
        "            estimators=voting_estimators,\n",
        "            voting='soft'\n",
        "        )\n",
        "        \n",
        "        # Stacking Classifier\n",
        "        stacking_estimators = voting_estimators\n",
        "        stacking_clf = StackingClassifier(\n",
        "            estimators=stacking_estimators,\n",
        "            final_estimator=GradientBoostingClassifier(\n",
        "                n_estimators=50,\n",
        "                random_state=self.random_state\n",
        "            ),\n",
        "            cv=5\n",
        "        )\n",
        "        \n",
        "        # Add ensemble models\n",
        "        self.models['Voting Ensemble'] = voting_clf\n",
        "        self.models['Stacking Ensemble'] = stacking_clf\n",
        "    \n",
        "    def evaluate_model_performance(self, X_train, X_test, y_train, y_test):\n",
        "        \"\"\"\n",
        "        Comprehensive model evaluation with multiple metrics\n",
        "        \"\"\"\n",
        "        print(\"üîç COMPREHENSIVE MODEL EVALUATION\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        results_df = []\n",
        "        \n",
        "        for name, model in self.models.items():\n",
        "            print(f\"\\nü§ñ Training {name}...\")\n",
        "            \n",
        "            try:\n",
        "                # Train model\n",
        "                model.fit(X_train, y_train)\n",
        "                \n",
        "                # Predictions\n",
        "                y_pred = model.predict(X_test)\n",
        "                y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "                \n",
        "                # Cross-validation\n",
        "                cv_scores = cross_val_score(\n",
        "                    model, X_train, y_train, \n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state),\n",
        "                    scoring='roc_auc'\n",
        "                )\n",
        "                \n",
        "                # Metrics calculation\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "                recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "                f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else 0\n",
        "                \n",
        "                # Store results\n",
        "                self.results[name] = {\n",
        "                    'model': model,\n",
        "                    'accuracy': accuracy,\n",
        "                    'precision': precision,\n",
        "                    'recall': recall,\n",
        "                    'f1_score': f1,\n",
        "                    'roc_auc': roc_auc,\n",
        "                    'cv_mean': cv_scores.mean(),\n",
        "                    'cv_std': cv_scores.std(),\n",
        "                    'y_pred': y_pred,\n",
        "                    'y_pred_proba': y_pred_proba\n",
        "                }\n",
        "                \n",
        "                results_df.append({\n",
        "                    'Model': name,\n",
        "                    'Accuracy': f\"{accuracy:.4f}\",\n",
        "                    'Precision': f\"{precision:.4f}\",\n",
        "                    'Recall': f\"{recall:.4f}\",\n",
        "                    'F1-Score': f\"{f1:.4f}\",\n",
        "                    'ROC-AUC': f\"{roc_auc:.4f}\",\n",
        "                    'CV Score': f\"{cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\"\n",
        "                })\n",
        "                \n",
        "                print(f\"‚úÖ {name} completed - ROC-AUC: {roc_auc:.4f}\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error training {name}: {str(e)}\")\n",
        "                continue\n",
        "        \n",
        "        # Create results DataFrame\n",
        "        results_df = pd.DataFrame(results_df)\n",
        "        \n",
        "        # Find best model based on ROC-AUC\n",
        "        if self.results:\n",
        "            best_model_name = max(self.results.keys(), \n",
        "                                key=lambda x: self.results[x]['roc_auc'])\n",
        "            self.best_model = {\n",
        "                'name': best_model_name,\n",
        "                'model': self.results[best_model_name]['model'],\n",
        "                'metrics': self.results[best_model_name]\n",
        "            }\n",
        "        \n",
        "        print(\"\\nüìä MODEL PERFORMANCE SUMMARY:\")\n",
        "        print(\"=\" * 80)\n",
        "        if not results_df.empty:\n",
        "            display(results_df)\n",
        "            print(f\"\\nüèÜ BEST MODEL: {self.best_model['name']}\")\n",
        "            print(f\"üéØ Best ROC-AUC: {self.results[self.best_model['name']]['roc_auc']:.4f}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No models completed successfully\")\n",
        "        \n",
        "        return results_df\n",
        "\n",
        "# Initialize and run the ML pipeline\n",
        "ml_pipeline = AdvancedMLPipeline()\n",
        "\n",
        "print(\"üöÄ ADVANCED ML PIPELINE INITIALIZATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Initialize models\n",
        "print(\"\\nü§ñ Initializing advanced models...\")\n",
        "ml_pipeline.initialize_models()\n",
        "\n",
        "# Create ensemble models\n",
        "print(\"üîó Creating ensemble models...\")\n",
        "ml_pipeline.create_ensemble_models()\n",
        "\n",
        "print(f\"‚úÖ Total models initialized: {len(ml_pipeline.models)}\")\n",
        "\n",
        "# Data preparation with robust scaling\n",
        "print(\"\\nüîß Preparing data with robust scaling...\")\n",
        "scaler = RobustScaler()\n",
        "X_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(X_selected),\n",
        "    columns=X_selected.columns,\n",
        "    index=X_selected.index\n",
        ")\n",
        "\n",
        "# Train-test split with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, \n",
        "    test_size=0.25, \n",
        "    random_state=42, \n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"üìä Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"üìä Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "# Run comprehensive evaluation\n",
        "performance_results = ml_pipeline.evaluate_model_performance(\n",
        "    X_train, X_test, y_train, y_test\n",
        ")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîç Model Interpretation and Clinical Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClinicalModelInterpreter:\n",
        "    \"\"\"\n",
        "    Advanced model interpretation with clinical relevance\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, X_test, y_test, feature_names):\n",
        "        self.model = model\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "        self.feature_names = feature_names\n",
        "    \n",
        "    def analyze_feature_importance(self):\n",
        "        \"\"\"\n",
        "        Analyze and visualize feature importance\n",
        "        \"\"\"\n",
        "        print(\"\\nüéØ FEATURE IMPORTANCE ANALYSIS\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        if hasattr(self.model, 'feature_importances_'):\n",
        "            # Tree-based model feature importance\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': self.feature_names,\n",
        "                'Importance': self.model.feature_importances_\n",
        "            }).sort_values('Importance', ascending=False)\n",
        "            \n",
        "            print(\"üèÜ TOP 10 MOST IMPORTANT FEATURES:\")\n",
        "            print(\"-\" * 40)\n",
        "            for i, (_, row) in enumerate(importance_df.head(10).iterrows(), 1):\n",
        "                print(f\"{i:2d}. {row['Feature']:<25} {row['Importance']:.4f}\")\n",
        "            \n",
        "            return importance_df\n",
        "        \n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è Feature importance not available for this model type\")\n",
        "            return None\n",
        "    \n",
        "    def clinical_risk_interpretation(self, importance_df):\n",
        "        \"\"\"\n",
        "        Provide clinical interpretation of important features\n",
        "        \"\"\"\n",
        "        print(\"\\n‚öïÔ∏è CLINICAL INTERPRETATION OF KEY RISK FACTORS\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        # Clinical interpretations for common features\n",
        "        clinical_interpretations = {\n",
        "            'cp': \"Chest pain type is a primary symptom indicator\",\n",
        "            'thalach': \"Maximum heart rate reflects cardiovascular fitness\",\n",
        "            'oldpeak': \"Exercise-induced ST depression indicates ischemia\",\n",
        "            'ca': \"Number of coronary vessels affected by stenosis\",\n",
        "            'thal': \"Thalassemia test results indicate blood flow patterns\",\n",
        "            'age': \"Age is a non-modifiable major risk factor\",\n",
        "            'sex': \"Gender influences cardiovascular disease presentation\",\n",
        "            'chol': \"Cholesterol levels affect arterial health\",\n",
        "            'trestbps': \"Resting blood pressure indicates vascular health\",\n",
        "            'exang': \"Exercise-induced angina suggests coronary insufficiency\",\n",
        "            'cv_risk_index': \"Composite cardiovascular risk score\",\n",
        "            'metabolic_risk_score': \"Combined metabolic dysfunction indicators\",\n",
        "            'hr_reserve': \"Heart rate reserve indicates fitness level\",\n",
        "            'exercise_capacity': \"Overall exercise tolerance assessment\"\n",
        "        }\n",
        "        \n",
        "        if importance_df is not None:\n",
        "            print(\"üîç TOP CLINICAL RISK FACTORS AND THEIR SIGNIFICANCE:\")\n",
        "            print(\"-\" * 60)\n",
        "            \n",
        "            for i, (_, row) in enumerate(importance_df.head(8).iterrows(), 1):\n",
        "                feature = row['Feature']\n",
        "                importance = row['Importance']\n",
        "                \n",
        "                # Find clinical interpretation\n",
        "                interpretation = \"Clinical significance being evaluated\"\n",
        "                for key, value in clinical_interpretations.items():\n",
        "                    if key in feature.lower():\n",
        "                        interpretation = value\n",
        "                        break\n",
        "                \n",
        "                print(f\"\\n{i}. {feature} (Importance: {importance:.4f})\")\n",
        "                print(f\"   üí° {interpretation}\")\n",
        "    \n",
        "    def generate_risk_assessment_summary(self):\n",
        "        \"\"\"\n",
        "        Generate comprehensive risk assessment summary\n",
        "        \"\"\"\n",
        "        print(\"\\nüìã CARDIOVASCULAR RISK ASSESSMENT SUMMARY\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        # Model performance summary\n",
        "        y_pred = self.model.predict(self.X_test)\n",
        "        y_pred_proba = self.model.predict_proba(self.X_test)[:, 1] if hasattr(self.model, 'predict_proba') else None\n",
        "        \n",
        "        accuracy = accuracy_score(self.y_test, y_pred)\n",
        "        precision = precision_score(self.y_test, y_pred, zero_division=0)\n",
        "        recall = recall_score(self.y_test, y_pred, zero_division=0)\n",
        "        roc_auc = roc_auc_score(self.y_test, y_pred_proba) if y_pred_proba is not None else 0\n",
        "        \n",
        "        print(\"üéØ MODEL PERFORMANCE METRICS:\")\n",
        "        print(\"-\" * 40)\n",
        "        print(f\"Accuracy: {accuracy:.1%} - Overall correct predictions\")\n",
        "        print(f\"Precision: {precision:.1%} - True positive rate among positive predictions\")\n",
        "        print(f\"Recall: {recall:.1%} - Sensitivity to detect heart disease cases\")\n",
        "        print(f\"ROC-AUC: {roc_auc:.3f} - Overall discriminative ability\")\n",
        "        \n",
        "        # Clinical recommendations\n",
        "        print(\"\\nüí° CLINICAL RECOMMENDATIONS:\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        if roc_auc >= 0.90:\n",
        "            print(\"üü¢ EXCELLENT: Model shows excellent predictive performance\")\n",
        "            print(\"   ‚Üí Suitable for clinical decision support\")\n",
        "            print(\"   ‚Üí Can effectively identify high-risk patients\")\n",
        "        elif roc_auc >= 0.80:\n",
        "            print(\"üü° GOOD: Model shows good predictive performance\")\n",
        "            print(\"   ‚Üí Useful for screening and risk stratification\")\n",
        "            print(\"   ‚Üí Recommend additional clinical validation\")\n",
        "        else:\n",
        "            print(\"üü† MODERATE: Model shows moderate predictive performance\")\n",
        "            print(\"   ‚Üí Requires further optimization\")\n",
        "            print(\"   ‚Üí Additional features or data may be needed\")\n",
        "        \n",
        "        print(\"\\n‚ö†Ô∏è IMPORTANT CLINICAL CONSIDERATIONS:\")\n",
        "        print(\"-\" * 40)\n",
        "        print(\"‚Ä¢ This model is for research and educational purposes\")\n",
        "        print(\"‚Ä¢ Clinical decisions should always involve qualified healthcare professionals\")\n",
        "        print(\"‚Ä¢ Model predictions should supplement, not replace, clinical judgment\")\n",
        "        print(\"‚Ä¢ External validation on diverse populations is recommended\")\n",
        "\n",
        "# Initialize clinical interpreter with best model\n",
        "if ml_pipeline.best_model:\n",
        "    best_model_name = ml_pipeline.best_model['name']\n",
        "    best_model_obj = ml_pipeline.best_model['model']\n",
        "\n",
        "    print(\"üîç ADVANCED MODEL INTERPRETATION & CLINICAL INSIGHTS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nüèÜ Analyzing best performing model: {best_model_name}\")\n",
        "\n",
        "    # Initialize clinical interpreter\n",
        "    clinical_interpreter = ClinicalModelInterpreter(\n",
        "        model=best_model_obj,\n",
        "        X_test=X_test,\n",
        "        y_test=y_test,\n",
        "        feature_names=X_selected.columns.tolist()\n",
        "    )\n",
        "\n",
        "    # Analyze feature importance\n",
        "    importance_results = clinical_interpreter.analyze_feature_importance()\n",
        "\n",
        "    # Generate clinical interpretation\n",
        "    clinical_interpreter.clinical_risk_interpretation(importance_results)\n",
        "\n",
        "    # Generate comprehensive risk assessment summary\n",
        "    clinical_interpreter.generate_risk_assessment_summary()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No best model available for interpretation\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìä Comprehensive Results Visualization Dashboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_comprehensive_results_dashboard():\n",
        "    \"\"\"\n",
        "    Create comprehensive visualization dashboard for model results\n",
        "    \"\"\"\n",
        "    if not ml_pipeline.best_model:\n",
        "        print(\"‚ö†Ô∏è No model results available for visualization\")\n",
        "        return None\n",
        "        \n",
        "    # Set up the plotting environment\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "    fig.suptitle('Advanced Heart Disease Prediction: Comprehensive Results Dashboard', \n",
        "                 fontsize=16, fontweight='bold', y=0.98)\n",
        "    \n",
        "    # 1. Model Performance Comparison\n",
        "    ax1 = axes[0, 0]\n",
        "    if ml_pipeline.results:\n",
        "        model_names = list(ml_pipeline.results.keys())\n",
        "        roc_scores = [ml_pipeline.results[name]['roc_auc'] for name in model_names]\n",
        "        \n",
        "        bars = ax1.barh(model_names, roc_scores, color='skyblue', alpha=0.8)\n",
        "        ax1.set_xlabel('ROC-AUC Score')\n",
        "        ax1.set_title('Model Performance Comparison\\n(ROC-AUC Scores)', fontweight='bold')\n",
        "        ax1.set_xlim(0, 1)\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for bar, score in zip(bars, roc_scores):\n",
        "            ax1.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n",
        "                    f'{score:.3f}', va='center', fontweight='bold')\n",
        "    \n",
        "    # 2. Confusion Matrix for Best Model\n",
        "    ax2 = axes[0, 1]\n",
        "    best_model_results = ml_pipeline.results[ml_pipeline.best_model['name']]\n",
        "    cm = confusion_matrix(y_test, best_model_results['y_pred'])\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
        "                xticklabels=['No Disease', 'Heart Disease'],\n",
        "                yticklabels=['No Disease', 'Heart Disease'])\n",
        "    ax2.set_title(f'Confusion Matrix\\n{ml_pipeline.best_model[\"name\"]}', fontweight='bold')\n",
        "    ax2.set_ylabel('True Label')\n",
        "    ax2.set_xlabel('Predicted Label')\n",
        "    \n",
        "    # 3. ROC Curve\n",
        "    ax3 = axes[0, 2]\n",
        "    if best_model_results['y_pred_proba'] is not None:\n",
        "        fpr, tpr, _ = roc_curve(y_test, best_model_results['y_pred_proba'])\n",
        "        roc_auc = best_model_results['roc_auc']\n",
        "        \n",
        "        ax3.plot(fpr, tpr, color='darkorange', lw=2, \n",
        "                label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
        "        ax3.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.5)\n",
        "        ax3.set_xlim([0.0, 1.0])\n",
        "        ax3.set_ylim([0.0, 1.05])\n",
        "        ax3.set_xlabel('False Positive Rate')\n",
        "        ax3.set_ylabel('True Positive Rate')\n",
        "        ax3.set_title(f'ROC Curve\\n{ml_pipeline.best_model[\"name\"]}', fontweight='bold')\n",
        "        ax3.legend(loc=\"lower right\")\n",
        "        ax3.grid(alpha=0.3)\n",
        "    \n",
        "    # 4. Feature Importance (if available)\n",
        "    ax4 = axes[1, 0]\n",
        "    if hasattr(ml_pipeline.best_model['model'], 'feature_importances_'):\n",
        "        importances = ml_pipeline.best_model['model'].feature_importances_\n",
        "        feature_names = X_selected.columns\n",
        "        importance_df = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'Importance': importances\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "        \n",
        "        top_features = importance_df.head(10)\n",
        "        bars = ax4.barh(top_features['Feature'], top_features['Importance'], \n",
        "                       color='lightcoral', alpha=0.8)\n",
        "        ax4.set_xlabel('Feature Importance')\n",
        "        ax4.set_title('Top 10 Feature Importance\\n(Clinical Risk Factors)', fontweight='bold')\n",
        "        \n",
        "        # Add value labels\n",
        "        for bar, importance in zip(bars, top_features['Importance']):\n",
        "            ax4.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2, \n",
        "                    f'{importance:.3f}', va='center', fontsize=8)\n",
        "    \n",
        "    # 5. Age vs Heart Disease Risk\n",
        "    ax5 = axes[1, 1]\n",
        "    age_disease = df_engineered.groupby(['age', 'target']).size().unstack(fill_value=0)\n",
        "    if 1 in age_disease.columns and not age_disease.empty:\n",
        "        age_risk_pct = age_disease.div(age_disease.sum(axis=1), axis=0)[1] * 100\n",
        "        \n",
        "        ax5.scatter(age_risk_pct.index, age_risk_pct.values, alpha=0.6, color='red')\n",
        "        ax5.set_xlabel('Age (years)')\n",
        "        ax5.set_ylabel('Heart Disease Risk (%)')\n",
        "        ax5.set_title('Age vs Heart Disease Risk\\n(Clinical Pattern)', fontweight='bold')\n",
        "        ax5.grid(alpha=0.3)\n",
        "        \n",
        "        # Add trend line\n",
        "        if len(age_risk_pct) > 1:\n",
        "            z = np.polyfit(age_risk_pct.index, age_risk_pct.values, 1)\n",
        "            p = np.poly1d(z)\n",
        "            ax5.plot(age_risk_pct.index, p(age_risk_pct.index), \"r--\", alpha=0.8, linewidth=2)\n",
        "    \n",
        "    # 6. Risk Stratification Summary\n",
        "    ax6 = axes[1, 2]\n",
        "    \n",
        "    # Create risk categories based on model predictions\n",
        "    if best_model_results['y_pred_proba'] is not None:\n",
        "        risk_proba = best_model_results['y_pred_proba']\n",
        "        risk_categories = pd.cut(risk_proba, \n",
        "                               bins=[0, 0.3, 0.7, 1.0], \n",
        "                               labels=['Low Risk', 'Moderate Risk', 'High Risk'])\n",
        "        risk_counts = risk_categories.value_counts()\n",
        "        \n",
        "        if not risk_counts.empty:\n",
        "            wedges, texts, autotexts = ax6.pie(risk_counts.values, \n",
        "                                              labels=risk_counts.index,\n",
        "                                              autopct='%1.1f%%',\n",
        "                                              colors=['lightgreen', 'gold', 'lightcoral'],\n",
        "                                              startangle=90,\n",
        "                                              explode=(0.05, 0.05, 0.05))\n",
        "            \n",
        "            ax6.set_title('Risk Stratification\\n(Model-Based)', fontweight='bold')\n",
        "            \n",
        "            # Enhance text appearance\n",
        "            for autotext in autotexts:\n",
        "                autotext.set_color('white')\n",
        "                autotext.set_fontweight('bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# Create comprehensive visualization dashboard\n",
        "print(\"üìä GENERATING COMPREHENSIVE RESULTS DASHBOARD\")\n",
        "print(\"=\" * 80)\n",
        "results_dashboard = create_comprehensive_results_dashboard()\n",
        "\n",
        "# Additional performance metrics visualization\n",
        "def create_detailed_metrics_plot():\n",
        "    \"\"\"\n",
        "    Create detailed metrics comparison plot\n",
        "    \"\"\"\n",
        "    if not ml_pipeline.results:\n",
        "        return None\n",
        "        \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Metrics comparison\n",
        "    metrics_data = []\n",
        "    for name, results in ml_pipeline.results.items():\n",
        "        metrics_data.append({\n",
        "            'Model': name,\n",
        "            'Accuracy': results['accuracy'],\n",
        "            'Precision': results['precision'],\n",
        "            'Recall': results['recall'],\n",
        "            'F1-Score': results['f1_score']\n",
        "        })\n",
        "    \n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "    \n",
        "    # Bar plot of metrics\n",
        "    x = np.arange(len(metrics_df))\n",
        "    width = 0.2\n",
        "    \n",
        "    ax1.bar(x - 1.5*width, metrics_df['Accuracy'], width, label='Accuracy', alpha=0.8)\n",
        "    ax1.bar(x - 0.5*width, metrics_df['Precision'], width, label='Precision', alpha=0.8)\n",
        "    ax1.bar(x + 0.5*width, metrics_df['Recall'], width, label='Recall', alpha=0.8)\n",
        "    ax1.bar(x + 1.5*width, metrics_df['F1-Score'], width, label='F1-Score', alpha=0.8)\n",
        "    \n",
        "    ax1.set_xlabel('Models')\n",
        "    ax1.set_ylabel('Score')\n",
        "    ax1.set_title('Detailed Performance Metrics Comparison')\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(metrics_df['Model'], rotation=45, ha='right')\n",
        "    ax1.legend()\n",
        "    ax1.grid(alpha=0.3)\n",
        "    \n",
        "    # Cross-validation scores\n",
        "    cv_means = [ml_pipeline.results[name]['cv_mean'] for name in ml_pipeline.results.keys()]\n",
        "    cv_stds = [ml_pipeline.results[name]['cv_std'] for name in ml_pipeline.results.keys()]\n",
        "    model_names = list(ml_pipeline.results.keys())\n",
        "    \n",
        "    ax2.barh(model_names, cv_means, xerr=cv_stds, alpha=0.8, capsize=5)\n",
        "    ax2.set_xlabel('Cross-Validation ROC-AUC Score')\n",
        "    ax2.set_title('Cross-Validation Performance (Mean ¬± Std)')\n",
        "    ax2.grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# Create detailed metrics plot\n",
        "print(\"\\nüìà GENERATING DETAILED METRICS COMPARISON\")\n",
        "print(\"-\" * 50)\n",
        "detailed_metrics_plot = create_detailed_metrics_plot()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
